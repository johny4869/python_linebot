{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1beeddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyAuEl_YtOrzUvn9P9RJ30z0oWUhAfwkQn0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ.get(\"GEMINI_API_KEY\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbac69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大型語言模型（LLM）的運作原理，可以從它「是什麼」、「如何學習」以及「如何生成」三個主要方面來理解。\n",
      "\n",
      "**核心思想：預測下一個詞（或詞元 Token）**\n",
      "\n",
      "最簡化的理解是：LLM是一個非常複雜且龐大的「預測器」。它會根據你給它的所有文字（稱為「上下文」），來推斷出接下來最有可能出現的下一個詞（或更準確地說是「詞元」，Token）。它不斷重複這個過程，每次預測一個詞元，然後將其加回上下文，再預測下一個，直到生成一段完整的、有意義的文本。\n",
      "\n",
      "想像它是一位超級淵博的作家，讀過世界上所有能找到的書籍和文章，並且對不同詞語如何組合、何種語境下表達何種意思了如指掌。當你給它一個開頭，它就能根據其「知識」流暢地續寫下去。\n",
      "\n",
      "### LLM 的主要構成與運作步驟：\n",
      "\n",
      "1.  **輸入處理：文字變數字 (Tokenization & Embedding)**\n",
      "    *   **分詞 (Tokenization)：** 首先，無論是你的輸入提示 (Prompt) 還是模型要生成的內容，文字都需要被分解成模型能理解的最小單元。這些單元通常不是單個字，也不是完整的詞，而是叫做「詞元」（Token）。例如，「unbelievable」可能被分成「un」、「believe」、「able」三個詞元。這樣做可以有效處理大量詞彙和新詞。\n",
      "    *   **嵌入 (Embedding)：** 接著，每個詞元會被轉換成一串數字，稱為「向量」。這些向量代表了詞元的語義信息。在一個高維空間中，意思相近的詞元，其向量會比較靠近。這一步是將人類語言轉換成機器可以計算的數學形式。\n",
      "\n",
      "2.  **模型核心：「大腦」 (Transformer Architecture)**\n",
      "    *   LLM 的「大腦」主要基於一種叫做 **Transformer** 的神經網路架構。這是其強大能力的基石。\n",
      "    *   **注意力機制 (Attention Mechanism)：** 這是 Transformer 最關鍵的部分。它允許模型在處理每個詞元時，能夠「回顧」並「權衡」輸入序列中所有其他詞元的重要性。\n",
      "        *   例如，在句子「他拿起筆寫字」中，當模型處理到「寫字」這個詞時，它會給「筆」這個詞更多的「注意力權重」，因為「筆」與「寫字」有直接的關聯。\n",
      "        *   這種機制使得模型能夠捕捉到長距離的語義依賴關係，理解句子中各部分之間的複雜聯繫，而不僅僅是緊鄰的詞語。\n",
      "\n",
      "3.  **學習過程：「讀書」與「修正」**\n",
      "    *   **預訓練 (Pre-training)：**\n",
      "        *   這是 LLM 學習基礎知識的階段。模型會在**海量的文本數據**上進行訓練（這些數據包括了書籍、文章、網頁、程式碼、對話記錄等，通常規模達到數萬億詞元）。\n",
      "        *   訓練任務通常是**預測下一個詞元**，或者在一個被遮蔽的句子中**填補缺失的詞元**。\n",
      "        *   透過這種無監督學習，模型學會了語言的語法、語義、事實知識、各種寫作風格、編程邏輯，甚至一些世界觀。這個階段的模型參數量巨大，儲存了巨量的「知識」。\n",
      "    *   **微調/指令微調 (Fine-tuning / Instruction Tuning)：**\n",
      "        *   預訓練的模型雖然知識淵博，但不一定擅長遵循特定指令或進行對話。\n",
      "        *   這個階段會在**較小、高品質的、人類編寫或篩選過的「指令-回應對」數據集**上進行訓練。例如：「指令：請總結以下文章。文章內容... 回應：文章總結...」\n",
      "        *   這讓模型學會理解並執行人類的指令，如回答問題、寫文章、翻譯、編程、總結等。\n",
      "    *   **人類回饋強化學習 (RLHF - Reinforcement Learning from Human Feedback)：**\n",
      "        *   這是讓模型行為更「像人」、更安全、更有幫助的關鍵步驟。\n",
      "        *   模型會生成多個回應，人類評審員會對這些回應進行排名或評分（例如：哪個答案更好、更安全、更沒有偏見）。\n",
      "        *   模型根據這些人類回饋進行「獎勵學習」，調整其內部參數，使其生成更符合人類偏好、更少有害內容的回應。這一步讓模型變得更「對齊」人類的期望。\n",
      "\n",
      "4.  **運作（推論）過程：生成回應**\n",
      "    *   當你向 LLM 提問（輸入 Prompt）時：\n",
      "        1.  你的 Prompt 會被分詞並轉換成嵌入向量。\n",
      "        2.  這些向量輸入到預訓練好的 Transformer 模型中。\n",
      "        3.  模型根據已有的上下文（即你的 Prompt），計算出接下來**所有可能的詞元**的**機率分佈**。例如，它可能會算出「是」的機率是 80%、「不」的機率是 15%、「也許」的機率是 5%。\n",
      "        4.  模型會根據這個機率分佈，**「選出」一個詞元**。通常不會直接選機率最高的，而是會引入一些隨機性（由「溫度」參數控制），以確保生成內容的多樣性和創造性，避免重複。\n",
      "        5.  選出的詞元會被加回到輸入序列的末尾，成為新的上下文。\n",
      "        6.  模型重複步驟 3 和 4，不斷預測並生成下一個詞元，直到生成足夠的內容，或者遇到特定的停止符號（例如句號、換行符等）。\n",
      "\n",
      "### LLM 的特性與能力：\n",
      "\n",
      "*   **大規模 (Large Scale)：** 數十億到數萬億的參數，數萬億詞元的訓練數據，這使其能夠捕捉到極為複雜的語言模式。\n",
      "*   **湧現能力 (Emergent Abilities)：** 當模型的規模達到一定程度時，會展現出一些在小規模模型中看不到的能力，例如複雜的推理、程式生成、多語言翻譯等。這些能力並非被明確編程，而是從大規模數據中「學會」的。\n",
      "*   **上下文理解 (Context Understanding)：** 能夠在處理長文本時表現出色，理解前文對後文的影響。\n",
      "*   **多任務能力 (Multi-task Capability)：** 經過微調後，可以執行多種不同類型的任務，而不需要為每個任務單獨訓練模型。\n",
      "\n",
      "### LLM 的局限性：\n",
      "\n",
      "*   **幻覺 (Hallucinations)：** LLM 有時會生成聽起來合理但實際上是錯誤或虛構的信息。這不是因為它在「說謊」，而是因為它只是在預測最可能的詞元序列，而不是真正「理解」事實或邏輯。\n",
      "*   **缺乏常識與推理能力：** 雖然能模仿複雜的推理過程，但其本質是基於訓練數據中的模式匹配，而非真正的邏輯推理或常識。它沒有真正的「意識」或「理解力」。\n",
      "*   **偏見 (Bias)：** 由於訓練數據來自現實世界，如果數據中存在偏見（例如性別、種族、地域偏見），模型很可能會繼承並放大這些偏見。\n",
      "*   **無法進行實時更新：** 模型的知識停留在其訓練數據的截止日期，無法獲取實時的新聞或事件。\n",
      "\n",
      "總結來說，LLM 的本質是一個極其複雜、強大的模式識別器和下一個詞元預測器。它通過學習海量數據中的語言規律，來生成連貫、相關且看似智能的文本，給人一種能夠「理解」和「思考」的錯覺。\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"解釋llm如何運作\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96bbd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe9518e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "身為一位INFP的軟體工程師，本業是資料科學家，想「偷師函數架構化」並「系統性地容納成自己的語言」是非常棒且切合INFP特質的目標！INFP重視深度、意義、個人成長和創造性表達，這些特質與函數式程式設計（Functional Programming, FP）的理念不謀而合。\n",
      "\n",
      "FP強調純粹、組合與可預測性，這能為你帶來一種「整潔」、「優雅」和「可信賴」的程式碼感受，這對追求意義和秩序的INFP來說，會非常有吸引力。\n",
      "\n",
      "以下為你量身打造的學習路徑和策略：\n",
      "\n",
      "---\n",
      "\n",
      "## INFP x FP：為什麼這是一場完美的結合？\n",
      "\n",
      "在開始之前，先認識FP如何與你的INFP特質共鳴，這會成為你學習的內在驅動力：\n",
      "\n",
      "1.  **追求純粹與意義 (Purity & Meaning):** FP的核心是「純粹函數」，沒有副作用，輸入相同就輸出相同。這會讓你感覺程式碼「誠實」且「可預測」，減少不確定性帶來的內耗。對INFP而言，這就像在混亂中找到秩序與清晰。\n",
      "2.  **創造性組合的樂趣 (Creative Composition):** FP鼓勵你將小而明確的函數像樂高積木一樣組合起來，解決複雜問題。這給你極大的自由去「設計」流程，而不是被動地遵循。INFP的創造力能在這裡得到很好的釋放。\n",
      "3.  **深度理解與掌控 (Deep Understanding & Control):** FP要求你更深入地思考數據流與轉換，而非僅僅操作狀態。這能滿足INFP對於事物本質的探究慾，讓你對程式碼的行為有更全面的掌握。\n",
      "4.  **減少認知負擔，專注於本質 (Reduce Cognitive Load):** 由於程式碼更可預測、副作用更少，你在除錯和思考時的認知負擔會降低。這能讓INFP將寶貴的精力投入到更有意義的抽象思考和問題解決上。\n",
      "\n",
      "---\n",
      "\n",
      "## 一、 理論基石：掌握核心概念（系統性容納的起點）\n",
      "\n",
      "身為資料科學家，Python會是你的主要工具，幸運的是Python對FP有很好的支援。\n",
      "\n",
      "1.  **純粹函數 (Pure Functions)：**\n",
      "    *   **定義：** 給定相同的輸入，總是返回相同的輸出，且不產生任何副作用（不修改外部狀態、不進行I/O操作）。\n",
      "    *   **INFP連結：** 想像它像一個可靠的實驗室儀器，每次測試結果都一致，不會因為外部環境而變化。這是你程式碼可靠性的基石。\n",
      "    *   **實例：**\n",
      "        ```python\n",
      "        def add(a, b): # 純粹函數\n",
      "            return a + b\n",
      "\n",
      "        x = 10\n",
      "        def increment_impure(): # 非純粹函數 (有副作用，修改外部變數)\n",
      "            global x\n",
      "            x += 1\n",
      "            return x\n",
      "        ```\n",
      "\n",
      "2.  **不可變性 (Immutability)：**\n",
      "    *   **定義：** 數據一旦創建就不能被修改。每次操作都返回一個新的數據副本。\n",
      "    *   **INFP連結：** 就像你的筆記，一旦寫下就不會被「偷偷」塗改，你可以放心地引用它。這讓你對數據流的追蹤更加清晰，減少變動帶來的焦慮。\n",
      "    *   **實例：** Python中的tuple, frozenset, 字符串都是不可變的。列表是可變的，但你可以練習返回新列表：\n",
      "        ```python\n",
      "        data = [1, 2, 3]\n",
      "        # Imperative (mutable)\n",
      "        # data.append(4)\n",
      "\n",
      "        # Functional (immutable style)\n",
      "        new_data = data + [4] # 创建一个新列表\n",
      "        ```\n",
      "\n",
      "3.  **頭等函數 (First-Class Functions)：**\n",
      "    *   **定義：** 函數可以像變數一樣被傳遞、賦值、作為參數傳入、作為返回值返回。\n",
      "    *   **INFP連結：** 你的「工具箱」裡，函數不再只是指令，它們本身就是你可以自由組合和操縱的「工具」。這擴展了你的創造空間。\n",
      "    *   **實例：**\n",
      "        ```python\n",
      "        def greet(name):\n",
      "            return f\"Hello, {name}!\"\n",
      "\n",
      "        my_func = greet # 函數可以被賦值給變數\n",
      "        print(my_func(\"Alice\"))\n",
      "\n",
      "        def apply_func(func, value): # 函數作為參數\n",
      "            return func(value)\n",
      "\n",
      "        print(apply_func(greet, \"Bob\"))\n",
      "        ```\n",
      "\n",
      "4.  **高階函數 (Higher-Order Functions, HOFs)：**\n",
      "    *   **定義：** 接受一個或多個函數作為參數，或返回一個函數作為結果的函數。\n",
      "    *   **INFP連結：** 你在創造更抽象的「流程」，讓這些流程可以適應不同的「細節操作」。這就像為你的數據管道設計一個靈活的骨架。\n",
      "    *   **實例：** Python內建的`map()`, `filter()`, `reduce()` 是最典型的HOFs，對數據科學尤為重要。\n",
      "        ```python\n",
      "        numbers = [1, 2, 3, 4]\n",
      "\n",
      "        # map: 对每个元素应用函数\n",
      "        squared = list(map(lambda x: x*x, numbers)) # [1, 4, 9, 16]\n",
      "\n",
      "        # filter: 根据条件筛选元素\n",
      "        evens = list(filter(lambda x: x % 2 == 0, numbers)) # [2, 4]\n",
      "\n",
      "        # reduce: 累积操作 (需要导入 functools)\n",
      "        from functools import reduce\n",
      "        sum_all = reduce(lambda acc, x: acc + x, numbers) # 10\n",
      "        ```\n",
      "\n",
      "5.  **函數組合 (Function Composition)：**\n",
      "    *   **定義：** 將多個函數連接起來，形成一個新的函數，一個函數的輸出是另一個函數的輸入。\n",
      "    *   **INFP連結：** 就像將你的樂高積木精巧地拼接成一個複雜的結構，形成一個流暢的「數據傳送帶」。你能夠清晰地追溯數據的整個轉換歷程。\n",
      "    *   **實例：** Python沒有內建的管道操作符（pipe operator），但可以通過庫實現或手動鏈接。\n",
      "        ```python\n",
      "        def add_one(x): return x + 1\n",
      "        def multiply_two(x): return x * 2\n",
      "\n",
      "        # 组合：先加1，再乘2\n",
      "        result = multiply_two(add_one(5)) # (5+1)*2 = 12\n",
      "        ```\n",
      "\n",
      "---\n",
      "\n",
      "## 二、 實踐路徑：「偷師」與應用（從資料科學本業入手）\n",
      "\n",
      "作為資料科學家，你的日常工作就是數據處理和分析，這本身就是一個巨大的FP學習場景！\n",
      "\n",
      "1.  **從現有程式碼開始重構 (Refactor your existing code)：**\n",
      "    *   這是「偷師」的最佳方式！挑選你目前工作中某個複雜的數據處理腳本或函數，問自己：\n",
      "        *   「這裡有哪些函數可以變成純粹函數？」\n",
      "        *   「我是否可以減少變數的修改，轉而返回新的數據結構？」\n",
      "        *   「哪些`for`迴圈可以用`map`、`filter`或`reduce`取代？」\n",
      "    *   **INFP連結：** 你在「優化」你自己的作品，這會給你帶來巨大的成就感。從現有基礎上改進，比從頭學一門新語言更符合INFP的學習曲線。\n",
      "\n",
      "2.  **數據管道的函數化 (Functional Data Pipelines)：**\n",
      "    *   Pandas的`apply()`, `pipe()`, `.assign()`, `.transform()` 等方法是實現FP思想的絕佳工具。\n",
      "    *   **練習：** 將一個包含多步驟轉換的Pandas數據框操作，分解成一系列純粹函數，然後用`.pipe()` 或方法鏈接組合起來。\n",
      "    *   **範例：**\n",
      "        ```python\n",
      "        import pandas as pd\n",
      "\n",
      "        df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
      "\n",
      "        def preprocess_col_A(df):\n",
      "            return df.assign(A=df['A'] * 2)\n",
      "\n",
      "        def normalize_col_B(df):\n",
      "            max_b = df['B'].max()\n",
      "            return df.assign(B=df['B'] / max_b)\n",
      "\n",
      "        # 使用函数组合 (pipe)\n",
      "        processed_df = (df\n",
      "                        .pipe(preprocess_col_A)\n",
      "                        .pipe(normalize_col_B))\n",
      "        print(processed_df)\n",
      "        ```\n",
      "    *   **進階：** 探索 `toolz` 或 `more-itertools` 這類為Python帶來更多FP工具的庫。\n",
      "\n",
      "3.  **模組化你的特徵工程 (Modularize Feature Engineering)：**\n",
      "    *   每個特徵生成或轉換的步驟都應盡可能寫成純粹函數。這樣可以獨立測試，並且易於組合和重用。\n",
      "    *   **INFP連結：** 就像為每個數據特性找到它「最優雅」的轉換方式，並將它們精心組織起來。\n",
      "\n",
      "4.  **小專案的全面實踐 (Full FP for Small Projects)：**\n",
      "    *   選擇一個全新的、小型的個人資料科學專案（例如：簡單的數據清洗、報告生成），從頭到尾嘗試用FP思想來構建。\n",
      "    *   刻意避免使用可變狀態和副作用。\n",
      "    *   **INFP連結：** 這是你的「實驗室」，你可以自由探索和創造，沒有現實工作的壓力。\n",
      "\n",
      "5.  **閱讀FP風格的開源程式碼 (Read FP-style Open Source Code)：**\n",
      "    *   尋找一些在Python中推崇FP風格的庫或專案，觀察他們是如何組織代碼的。例如，一些處理數據流的庫或 ETL 工具可能會有FP的影子。\n",
      "    *   **INFP連結：** 觀察「大師」的作品，從中汲取靈感和模式，這就像一種無聲的交流。\n",
      "\n",
      "---\n",
      "\n",
      "## 三、 系統化與內化：「自己的語言」形成之路\n",
      "\n",
      "學習一個新範式，不僅是學會語法，更是形成一套新的思維模式。\n",
      "\n",
      "1.  **建立你的「概念地圖」或「詞彙表」 (Build Your Concept Map/Glossary)：**\n",
      "    *   用你自己的話，解釋每個FP概念（純粹函數、不可變性、高階函數等）。\n",
      "    *   思考這些概念如何與你熟悉的命令式程式設計（Imperative Programming）或物件導向程式設計（OOP）對應或區分。\n",
      "    *   **INFP連結：** 你在「翻譯」這些概念，使其融入你的內在理解框架。這種個性化的整理方式，對INFP尤其有效。\n",
      "\n",
      "2.  **為問題尋找「函數式」解法 (Find \"Functional\" Solutions for Problems)：**\n",
      "    *   遇到一個新的程式設計問題時，不要急著寫命令式代碼。先問自己：「如果用純粹函數和不可變性，我會怎麼解決？」\n",
      "    *   試著從數據流的角度去思考：數據經過哪些轉換？每個轉換的輸入和輸出是什麼？\n",
      "    *   **INFP連結：** 這是一種「設計思維」，讓你深入探究問題的本質，並找到更優雅的解決方案。\n",
      "\n",
      "3.  **內部代碼審查：用FP的眼睛看代碼 (Internal Code Review with FP Lens)：**\n",
      "    *   定期回顧自己寫的程式碼（無論新舊），問自己：\n",
      "        *   「這個函數是不是純粹的？如果不是，如何讓它更純粹？」\n",
      "        *   「哪裡有狀態被無意中修改了？能否用不可變的方式重寫？」\n",
      "        *   「我是否可以將這些操作鏈接起來，形成更清晰的數據流？」\n",
      "    *   **INFP連結：** 這是自我完善的過程，通過不斷的審視和優化，你對「好代碼」的標準會越來越清晰。\n",
      "\n",
      "4.  **類比與比喻 (Analogies and Metaphors)：**\n",
      "    *   INFP善於抽象思考和使用比喻。嘗試為FP的各種概念創造你自己的類比。\n",
      "    *   例如：純粹函數像「黑盒子工廠」，輸入原料固定，產出也固定；數據管道像「流水線」；函數組合像「串聯的濾水器」。\n",
      "    *   **INFP連結：** 這些個人化的類比會幫助你更好地記憶和理解複雜概念，並將其根植於你的直覺。\n",
      "\n",
      "5.  **解釋給別人聽（即使是假想的聽眾）(Explain to Others (or Imaginary Audience))：**\n",
      "    *   當你認為自己理解了一個FP概念時，試著向一個沒有程式設計背景的人解釋它。如果你能用簡單、清晰的語言讓他們理解，那麼你自己的理解就非常紮實了。\n",
      "    *   **INFP連結：** 教學是最好的學習方式。即使是對著空氣自言自語，也能幫助你將模糊的知識點具體化。\n",
      "\n",
      "---\n",
      "\n",
      "## 四、 INFP專屬學習策略\n",
      "\n",
      "1.  **從「為什麼」開始，而非「怎麼做」：** INFP需要意義。理解FP背後的哲學和它解決的問題（可測試性、並行性、錯誤減少等），會比單純記憶語法更有動力。\n",
      "2.  **允許深度探索：** 當你對某個概念產生興趣時，允許自己深入挖掘，即使這會讓你暫時偏離主線。這種深度沉浸是INFP學習的強項。\n",
      "3.  **注重過程而非結果：** 不要期望一夜之間成為FP大師。享受將現有程式碼逐步「函數化」的過程，感受每次優化帶來的清晰感。\n",
      "4.  **創造你自己的學習資源：** 寫部落格、做筆記、畫概念圖。這些「創造性輸出」會幫助你鞏固學習，並形成你獨特的知識體系。\n",
      "5.  **尋找共鳴社群（謹慎選擇）：** 一些FP社群可能偏向學術或理論，可能不是INFP最初的舒適區。但當你對某些概念有疑問時，尋找友善且鼓勵的線上社群（如Python的數據科學或FP討論組），問問題，分享你的心得。\n",
      "6.  **接受不完美：** 剛開始，你的「函數式」程式碼可能不那麼完美。沒關係，INFP容易陷入完美主義，但學習是一個迭代的過程。每次嘗試都是進步。\n",
      "\n",
      "---\n",
      "\n",
      "## 五、 推薦資源\n",
      "\n",
      "*   **Python內建模組：**\n",
      "    *   `functools`: 提供高階函數工具，如`partial`（部分應用）、`wraps`（裝飾器）、`reduce`等。\n",
      "    *   `itertools`: 用於高效迭代，很多操作是惰性求值的，符合FP思想。\n",
      "*   **Python FP庫：**\n",
      "    *   **`toolz`**: 集合了許多實用的函數式工具，包括函數組合、柯里化、數據轉換等。\n",
      "    *   **`pipe`**: 提供了一個更類似Unix管道的操作符 `|`，讓函數組合更直觀。\n",
      "    *   `more-itertools`: 擴展了`itertools`的功能，提供了更多有用的迭代器工具。\n",
      "*   **書籍：**\n",
      "    *   《Functional Python Programming》（英文）：專為Python開發者設計，從命令式過渡到函數式。\n",
      "    *   《Grokking Functional Programming》（英文）：以更直觀的方式解釋FP概念，適合初學者。\n",
      "*   **線上課程/部落格：**\n",
      "    *   搜尋「Functional Programming in Python for Data Science」或「Python Data Pipeline Functional Style」，會有很多實用的文章和教程。\n",
      "    *   留意一些Python數據科學領域的Kaggle notebook，有些作者會採用更函數式的風格。\n",
      "*   **考慮學習一門純粹的函數式語言（進階）：** 當你對Python的FP掌握得差不多，如果想進一步深化理解，可以淺嘗 Haskell, Clojure 或 Scala。它們會強迫你用FP的方式思考，對鞏固概念非常有幫助。但這不是必需的，Python本身就足以讓你將FP應用到日常工作中。\n",
      "\n",
      "---\n",
      "\n",
      "**總結：**\n",
      "\n",
      "身為INFP，你擁有深度思考、追求意義和創造性的天賦。將這些特質與函數式程式設計的純粹、組合和可預測性結合，將會為你的軟體工程和資料科學實踐帶來新的維度。從Python和你的資料科學工作切入，一步步重構、練習、內化，你會發現這不僅是一種新的編碼方式，更是一種讓你程式碼更優雅、更可靠的「思維藝術」。祝你學習愉快，享受這段尋求程式碼「意義」的旅程！\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"身為一個INFP的軟體工程師且本業是資料科學怎樣方式偷師函數架構化!以及系統性的容納成自己的語言呢\"\n",
    ")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linebot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
